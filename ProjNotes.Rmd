---
title: "Project Zero"
output: html_notebook
---
## Project Notes 

This project is concerned with looking at evolutionary signatures of selection on immune-associated genes, both canonical and candidate. All immune genes (canon/noncanon) are based on orthologs from Apis mellifera (Amel_HAv3.1, NCBI genome).

Two types of analyses have been previously ran using PAML's module, codeml, using gene product codon alignments consisting of 11 bee species, including Apis mellifera. Gene sets are divided into "immunity" and "random", with immunity consisting of "canon" and "non-canon". All alignments consist of 1-1 single-copy orthologs present in all 11 species. Those that did not include all species were dropped.

Evolutionary rate of each gene product was estimated using a codeml run that considers the whole gene product across the given phylogeny as a whole. It is not expected to get dN/dS ratio (omega) values anywhere near 1 (positive selection) using this method as site-specific signals of selection will be likely lost as the ratio is averaged along the length of the product. Instead, this is expected to offer a relative scale of evolutionary rate.

The main analysis consisted of branch-site specific analyses of selection wherein the phylogenetic tree was designated a target, splitting the tree into foreground and background. Codeml then estimates omega value for foreground branches relative to background. For the purposes of this project, each of the designations correspond with the origin or elaboration of social living, to assess if and how these changes in lifestyle affected selection on immune and candidate immune genes in the bees, relative to other genes (the "random" set, which encompass all orthologs that could be made from all proteins in Apis mellifera using the ortholog bash script, OrthoScript [imaginitively named]).

A list of all of the designations and the input newick formats is available in Genome_Misc/tree_designations.txt. I.e.

```{r}
cat(readLines('Genome_Misc/tree_designations.txt', n=2), sep="\n")
```


In the null version of this test, omega ratios on the foreground branches are fixed at 1, with the alternative version starting with omega estimates above 1 (a direct test of positive selection). Each model results in a global maximum log likelihoood score which can then be compared using a LRT.

LRT = (lnLAlt - lnLNull)*2

LRT scores can then be compared to a chi-square distribution with 1 degree of freedom to assess significance. LRT scores above 3.84 are considered significant. 

P-values will be adjusted using during this analysis to correct for multiple testing using the Benjamini-Hochburg procedure.

##Setting the Verse

In order to annotate the data with class of gene, description of gene (as it is described in Apis mellifera), etc. we first need a "dataverse" of Amel_HAv3.1.

verseSet.R reads in the relevant information from the Genome_Misc and ImmResources folders.

```{r}
source("Scripts/verseSet.R", local = knitr::knit_global())
```


GeneVerse: created by reading in a file with all genes from Amel HAv3.1 complete with gene product description and transcript ID.

```{r}
head(gene.verse)
```

ImmVerse: created by reading in a file with all immune - associated genes used as the input for the orthologue finding program which created the codon alignments used in the codeml branch-site selection analyses, complete with classifications, transcript, peptide and gene IDs.

```{r}
head(imm.verse)
```

##Immunity Dataset

For each of the 9 origins/elaborations (and episodic versus longterm shift in selection), a results file is produced (using my own batch-PAML running script, BranchSite_Codeml_V2.sh, and the various associated .ctl and tree files). 

Each lnLResults.txt file consists of the name of the gene, the log likelihood score of the null and alternative branch-site models (lnL_Null / lnL_Alt) and the results of the likelihood ratio test ran between them (LRT).

These raw data files look like this.

```{r}
cat(readLines('input/immunity/AllComplex.3.lnLResults.txt', n=5), sep = '\n')
```

###Adding annotations

dataRead_immunity.R will read in the results file, collate them into one source and annotate each gene with descriptions, class of immunity (canon / non-canon) and social origin/elaboration.

```{r}
source("Scripts/dataRead_immunity.R", local = knitr::knit_global())
```

```{r}
head(data[[2]])
```

###Tests of significance

ChiSig.R uses the LRT scores and a Chi square distribution (one-tailed, one degree of freedom) to produce pvalues. Pvalues below 0.05 are considered significant (before correction for multiple testing).

To correct for multiple testing, I'll be using the Benjamini-Hochburg procedure.


```{r}
source("Scripts/SigTests.R", local = knitr::knit_global())
```

```{r}
head(data[[3]])
```

How many are still considered significant ? 

```{r}
data.imm <- bind_rows(data)
data.imm$SocOrigin <- as.factor(data.imm$SocOrigin)
data.sig.imm <- filter(data.imm, adj_pvalue < 0.05)
summary(data.sig.imm$SocOrigin)
```

###Write Up

```{r}
write.table(data.imm, "output/CodeML_Results_AllImmune.tsv", quote = F, row.names = F, sep = "\t")
write.table(data.imm, "output/CodeML_SigResults_AllImmune.tsv", quote = F, row.names = F, sep = "\t")
```

##Random Dataset

I'm going to repeat the above, but all at once (using a script, dataReadandTest_random.R).

"Data"" will become a list of the random codeML runs, with it also being saved separated in data.ran.

```{r}
source("Scripts/dataReadandTest_random.R", local = knitr::knit_global())
```

```{r}
head(data[[4]])
```

## Compare Immunity and Random

First off I'm going to combine the random and immune data and put them back into a list of dataframe environment.

```{r}
data <- rbind(data.imm, data.ran)
write.table(data, "output/CodeML_BranchSiteAnalysis_CompleteResults.tsv", sep="\t", row.names=F, quote=F)

data <- split(data, f = data$SocOrigin)
summary(data[[5]])
```

###Fisher's Exact Test

I will be running a series of Fisher's exact tests (2x2 contingency tables) per social origin: 1 imm versus random, 2 can versus random, 3 noncan versus random, 4 can versus noncan.

Each origin will have its own contingency table per test consisting of a [1,1][1,2],[2,1][2,2] matrix. Colnames are outcomes Selection and No Selection (i.e, significant or not), and the groups are as defined above, in the order defined above. 

For example, CorbSoc test of Can versus Random test will have data input of:

[1,1] = no of can under selection

[2,1] = no of can with no selection (total can - above)

[2,1] = no of random under selection

[2,2] = no of random with no selection (total random - above)

I'll be running a script to achieve the above looking at unadjusted pvalues that are considered significant and spit it out in an object called p.results.

```{r}
source("Scripts/pFisher.R", local = knitr::knit_global())
```

```{r}
p.results
```

A couple of some near significant differences, but no hits otherwise. 

And for the adjusted pvalues ...

```{r}
source("Scripts/adjFisher.R", local = knitr::knit_global())
```

```{r}
adj.results
```

Ha! Lots less significant.

I will have to double check if the Fisher's test is still appropriate now that the size of the random dataset is so much bigger than it was originally.

-- It's not. Sample size is too large. Pearson's Chisq is probably  more apt.

###Pearson's ChiSq with Simulated Pvalues

Using the same contigency tables used above, I will be running ChiSq tests of significance. As in some of these cases the sample frequencies are heavily imbalanced (i.e 5 canonical immune genes under selection versus ~6000 random genes not under selection), I will be simulating the pvalue by Monte Carlo simulation (x10000). 

```{r}
source("Scripts/adjChi.R", local = knitr::knit_global())
```

```{r}
chi.adj.results
```

In terms of significant relationships, I think this is a no.

###Bootstrapping Approach

Bootstrapping is a resampling technique to estimate statistics on a population by sampling a dataset with replacement.

A useful feature of the bootstrap method is that the resulting sample of estimations often forms a Gaussian distribution. In additional to summarizing this distribution with a central tendency, measures of variance can be given, such as standard deviation and standard error. Further, a confidence interval can be calculated and used to bound the presented estimate. This is useful when presenting the estimated skill of a machine learning model. (https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/)

Sample Size - use a sample size that is the same size as the original dataset. For my purposes, this will be the number of genes under selection (all classes) per origin of sociality tested. The statistic I'm looking for is the number of genes from each class that are then found in each randomly selected dataset. 

i.e. using the CorbSoc origin of sociality ...

```{r}
test <- data[["CorbSoc"]]
nrow(test[test$adj_pvalue < 0.05,])
```

This means I would be making a "population" of all the genes that were tested (this will be very slightly different for each origin of sociality as some genes failed in some origins that worked in others, etc) and then sampling 12 genes at random. The number of canon or noncanon genes will be returned, before the genes are replaced and sampling occurs again, x100,000.

From this I should get a distribution with other descriptive statistics including confidence intervals, against which I can then say whether the number of genes of each class under selection differ from what can be expected by chance (by random sampling).



